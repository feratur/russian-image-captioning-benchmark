# A survey of models for generating image captions in Russian
A small benchmark of State-of-the-art Deep Learning models for generating Russian annotations for images.

## Comparison method
The most widely used metrics for evaluating the performance of models on Image Captioning task are BLEU, METEOR, ROUGE, CIDEr and SPICE. All of them (excluding SPICE) mainly measure the overlap of words between generated and reference captions. SPICE is slightly different - it requires parsing the sentences as graphs and calculating F-score over tuples in the candidate and reference scene graphs.

The results produced by evaluation using the metrics above are usually either random and unstable or require a large amount of generated captions for building confident statistics - but the price is considerable computational resources.

My proposed method is to use [RuCLIP](https://github.com/ai-forever/ru-clip) model (`ruclip-vit-base-patch32-384`, the best available [CLIP](https://arxiv.org/abs/2103.00020)-like model in Russian) for evaluating semantic similarity between images and their captions in Russian. Even though this method introduces additional bias and relies heavily on the quality of the said model, it allows for less randomized measurements and more reliable comparison as the generated scores are well-distributed and tend to correlate more with human judgement.

## About the dataset
There is no existing Russian open-source image-caption pair dataset for comparing the performance of different models on Image-to-Text task, so I used a standard MSCOCO dataset ([[Link]](https://cocodataset.org/)) and translated original captions into Russian.

I used val2017 split - 5000 images, each image has around 5 different annotation variants. All annotations were translated into Russian using **Helsinki-NLP Opus-MT en-ru** translation model ([github](https://github.com/Helsinki-NLP/Opus-MT), [huggingface](https://huggingface.co/Helsinki-NLP/opus-mt-en-ru)). Then translated annotations were ranked using [RuCLIP](https://github.com/ai-forever/ru-clip) and for each image the annotation with the highest cosine similarity score was selected as the ground truth.

## How to reproduce
All artifacts (RuCLIP embeddings, generated and translated annotations, scores), generated during the benchmark, can be found in `artifacts` directory. The file `artifacts/ruclip_final_scores.json` contains the final cosine similarity scores between COCO images and Russian annotations, generated by compared models (and original COCO captions, translated into Russian) - as evaluated by [RuCLIP](https://github.com/ai-forever/ru-clip).

To reproduce the results a Docker-enabled machine with NVIDIA GPU (at least 16 GB of VRAM and CUDA 11.3 [compatible](https://docs.nvidia.com/deploy/cuda-compatibility/index.html) drivers are required) can be used:
```
git clone git@github.com:feratur/russian-image-captioning-benchmark.git
cd russian-image-captioning-benchmark
docker run --rm -it --gpus all -v $(pwd)/artifacts:/workspace/artifacts $(docker build -q .)
```
The process includes downloading the dataset, downloading models and their checkpoints, generating and translating (when applicable) image captions and evaluating cosine similarity scores between COCO image embeddings and annotation embeddings using [RuCLIP](https://github.com/ai-forever/ru-clip). The whole process may take up to a day on a machine with NVIDIA T4 GPU.

After the evaluation process is finished the files in `artifacts` directory will be overwritten with newly generated ones.

## Conclusions and future work

### Histogram of ruCLIP scores for compared models
![Alt text](artifacts/hist.png?raw=true "Comparison histogram")

